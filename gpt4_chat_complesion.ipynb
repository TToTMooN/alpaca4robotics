{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_robot_instruction import encode_task_generation_prompt, generate_task_data, encode_instruct_prompt, generate_instruction_following_chat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"You are serving as a task-generation helper for a given robot environment.\\xa0\\n\\nHere is the information about the environment. The environment is called {Two Block World}\\xa0\\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe blocks are initialized at a random position on a table.\\nContents in the {} are the names of the objects in the environment.\\n\\nCome up with 10 different tasks for the robot to perform.\\n\\nGenerate the response following the template below:\\n### Task {i}: {task description}\\nwhere {i} is the task number and {task description} is the task description.\\n\\n\\nAn example task description is: {pick up the heavier block and place it on top of the lighter block}\\n\\nThe rules for task description:\\n1. Only include the objects in the environment in the task description.\\n2. The task description doesn't need to include all the objects in the environment.\\n3. You can assume the robot can do basic manipulation skills with a single parallel gripper.\\n4. The task description can be implicit in the objects. For example, {Pick up the heavier block} is a valid task description.\\n5. The task description can be implicit in the goal. For example, {Maximize the height by stacking the two blocks} is a valid task description.\\n6. Use your imagination to come up with different tasks. The tasks should be diverse and not too similar to each other.\\n7. The observation of the robot is the position and orientation of both blocks and the end effector of the robot, and the force sensor on the end effector. Try to devise tasks that are not trivial to solve with the initial observation. In other words, there are uncertainties in the task that requires the robot to explore the environment to solve the task. For tasks you think satisfy this requirement, please add a * at the end of the task description. For example, {pick up the heavier block and place it on top of the lighter block}*.\\n\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = encode_task_generation_prompt()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=message,\n",
    "    temperature=1.0,\n",
    "    top_p=1,\n",
    "    max_tokens=512,)\n",
    "response = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x7f8dd9558ef0> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"### Task 1: Pick up block B and place it directly next to block A without altering the initial orientation of block A*.\\n### Task 2: Manipulate block A and block B such that they are on the opposite sides of the table*.\\n### Task 3: Maximize the height by stacking the two blocks on top of each other*.\\n### Task 4: Pick up the lighter block and place it on the ground beneath the table without touching the heavier block.\\n### Task 5: Rotate block A 90 degrees without moving it from its initial position*.\\n### Task 6: Identify which block is denser, then lift that block and place it on top of the other block*.\\n### Task 7: Reorient blocks A and B such that they are at 180 degrees from each other with the heaviest block on the table and the lighter block on top of it*.\\n### Task 8: Move both block A and block B to the corners of the table.\\n### Task 9: Arrange blocks A and B side by side with a gap of at least 5cm between them*.\\n### Task 10: Determine which block is taller and place the shorter block on top of the taller block*.\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_task_response(response):\n",
    "    if response is None:\n",
    "        return []\n",
    "    ### Parse the response into list of tasks ###\n",
    "    raw_tasks = re.split(\"###\", response)\n",
    "    print(raw_tasks)\n",
    "    tasks = []\n",
    "    for idx, task in enumerate(raw_tasks):\n",
    "        # # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
    "        # if idx == len(raw_tasks) - 1 and response[\"finish_reason\"] == \"length\":\n",
    "        #     continue\n",
    "        ##### Parse the response into task #####\n",
    "        splitted_data = re.split(f\"{idx}:\\s+\", task)\n",
    "        if len(splitted_data) != 2:\n",
    "            continue\n",
    "        else:\n",
    "            task = splitted_data[1].strip()\n",
    "\n",
    "        ##### FILTER OUT Negative Examples #####\n",
    "        # filter out too short or too long tasks\n",
    "        if len(task.split()) <= 3 or len(task.split()) > 150:\n",
    "            continue\n",
    "        # filter based on keywords that are not suitable for language models.\n",
    "        # filter those starting with punctuation\n",
    "        if task[0] in string.punctuation:\n",
    "            continue\n",
    "        # filter those starting with non-english character\n",
    "        if not task[0].isascii():\n",
    "            continue\n",
    "        tasks.append(task)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_task_data(num_tasks_to_generate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 2 column 1 (char 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generate_instruction_following_chat_data(num_instructions_to_generate\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/codebase/alpaca4robotics/generate_robot_instruction.py:224\u001b[0m, in \u001b[0;36mgenerate_instruction_following_chat_data\u001b[0;34m(output_dir, seed_tasks_path, seed_example_path, function_file_path, num_instructions_to_generate, model_name, num_prompt_instructions, request_batch_size, temperature, top_p, num_cpus)\u001b[0m\n\u001b[1;32m    222\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    223\u001b[0m request_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 224\u001b[0m seed_tasks \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(seed_tasks_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m    225\u001b[0m seed_instructions \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(seed_example_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m    226\u001b[0m functions \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(function_file_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/codebase/alpaca4robotics/generate_robot_instruction.py:224\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    222\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    223\u001b[0m request_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 224\u001b[0m seed_tasks \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39;49mloads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(seed_tasks_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m    225\u001b[0m seed_instructions \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(seed_example_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m    226\u001b[0m functions \u001b[39m=\u001b[39m [json\u001b[39m.\u001b[39mloads(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mopen\u001b[39m(function_file_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 2 column 1 (char 2)"
     ]
    }
   ],
   "source": [
    "generate_instruction_following_chat_data(num_instructions_to_generate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_tasks_path=\"./prompts/seeded_tasks.jsonl\"\n",
    "seed_example_path=\"./prompts/seeded_example.jsonl\"\n",
    "function_file_path=\"./prompts/skill_functions.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
    "seed_instructions = [json.loads(l) for l in open(seed_example_path, \"r\")]\n",
    "functions = [json.loads(l) for l in open(function_file_path, \"r\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='./prompts/seeded_tasks.jsonl' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(seed_tasks_path, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = []\n",
    "for _ in range(1):\n",
    "    # only sampling from the seed tasks\n",
    "    # prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)\n",
    "    prompt = encode_instruct_prompt(\n",
    "        tasks=seed_tasks,\n",
    "        functions=functions,\n",
    "        examples=seed_instructions,\n",
    "    )\n",
    "    batch_input.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 20 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper. \\nThere are 2 blocks, {block A} and {block B} with randomized sizes and density in the environment.\\nThe blocks are initialized at a random position on a table.\\nContents in the {} are the names of the objects in the environment.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Methodically stack block A on top of block B regardless of their weight, in an attempt to maximize their combined height.\\'}\\n3. {\\'task\\': \"Pick and hold block A and shake it gently without dropping it to understand the nature and density of the block\\'s material.*\"}\\n4. {\\'task\\': \\'Using the force sensor, apply a specific amount of force against block B, moving it a certain distance across the table.\\'}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The format of response will list of instructions {i} <instruction>{instruction}\\\\n <input>{input}\\\\n <output>[verbal]{verbal output}[action]{function output}\\n4. The format of {instruction} will be a question.\\n5. The format of {input} will be a vector of robot observation.\\n6. The format of {verbal output} will be a sentence.\\n7. The format of {action output} will be {function name} {function parameter}. If the function has no parameter, you can use <noparam> to represent.\\n\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n###\\n###\\n###\\n###\\n###\\n Task: stack block A on block B\\n Instruction: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n Input:\\n[[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n Output:\\n<verbal output>Yes, the current information is enough to plan and solve the task. <action output>[grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n Task: stack the heavier on the ligher block\\n Instruction: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n Input:\\n[[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n Output:\\n<verbal output>No. We don\\'t know the weight of the blocks, need to infer weight first. <action output>[grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [00:49<00:00, 49.26s/it]\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "decoding_args = utils.OpenAIChatDecodingArguments(\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=2048,\n",
    "    stop=[\"\\n10\", \"10.\"],\n",
    ")\n",
    "chatcompletions = utils.openai_chatcompletion(\n",
    "    prompts=batch_input,\n",
    "    model_name=\"gpt-4\",\n",
    "    decoding_args=decoding_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"###\\nTask: Identify and pick up the lighter block and move it to the opposite end of the table\\nInstruction 1: Is it possible to identify the lighter block by looking at them?\\nInput: <no input>\\nOutput:\\n<verbal output>No, we need to grasp each block and use the force sensor to infer the weight of the blocks.<action output>grasp(block A), grasp(block B).\\n\\nInstruction 2: Based on the sensor feedback, which is the lighter block? \\nInput: [Force reading from blockA, Force reading from blockB]\\nOutput:\\n<verbal output>Block A is lighter based on the force sensor readings.<action output>grasp(block A).\\n\\nInstruction 3: What is the next step after identifying the lighter block?\\nInput: <no input>\\nOutput:\\n<verbal output>Move the lighter block to the opposite end of the table.<action output>place(block A, [opposite end of the table position]).\\n\\n###\\nTask: Methodically stack block A on top of block B regardless of their weight, in an attempt to maximize their combined height\\nInstruction 1: Can you identify the current positions of block A and block B? \\nInput: [Position of Block A and Block B]\\nOutput:\\n<verbal output>Yes, based on the current observation, block A is at position X and block B is at position Y.<action output>reach(block A position), reach(block B position).\\n\\nInstruction 2: Can you stack block A on top of block B?\\nInput: [Position of Block A, Position of Block B]\\nOutput:\\n<verbal output>Yes, grasp block A, move it over block B and release it.<action output>grasp(block A), place(block A, [block B position + block B height]).\\n\\nInstruction 3: Did the stacking occur successfully? \\nInput: [Relative positions of Block A and Block B after the stacking attempt]\\nOutput:\\n<verbal output>Yes, block A was successfully stacked on top of block B.<action output>grasp(block B). \\n\\n###\\nTask: Pick and hold block A and shake it gently without dropping it to understand the nature and density of the block's material\\nInstruction 1: Can you pick up block A?\\nInput: [Position of Block A]\\nOutput:\\n<verbal output>Yes, I can reach block A and pick it up.<action output>grasp(block A).\\n\\nInstruction 2: Can you shake block A gently without dropping it?\\nInput: <no input>\\nOutput:\\n<verbal output>Yes, I will have to hold the block firmly while performing the action.<action output>move_to(shake position), move_to(block A original position).\\n\\n###\\nTask: Using the force sensor, apply a specific amount of force against block B, moving it a certain distance across the table\\nInstruction 1: Can the force sensor read the amount of force being applied?\\nInput: <no input>\\nOutput:\\n<verbal output>Yes, the force sensor can measure the force.<action output>reach(block B position).\\n\\nInstruction 2: Can you apply a specific amount of force against block B?\\nInput: <no input>\\nOutput:\\n<verbal output>Yes, I can apply a specified amount of force.<action output>move_to(block B position + [desired displacement vector]).\\n\\nInstruction 3: Did the block move the desired distance?\\nInput: [Position of Block B after application of force]\\nOutput:\\n<verbal output>Yes, block B moved the desired distance.<action output><noparam>.\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatcompletions[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "201e8860b8e6658cdfe301dc13e2a8b5ecdc17e6f5f25d00e326e3c8c2d13630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
