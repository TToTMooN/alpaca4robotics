{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_robot_instruction import encode_task_generation_prompt, generate_task_data, encode_instruct_prompt, generate_instruction_following_chat_data, post_process_chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"You are serving as a task-generation helper for a given robot environment.\\xa0\\n\\nHere is the information about the environment. The environment is called {Two Block World}\\xa0\\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe blocks are initialized at a random position on a table.\\nContents in the {} are the names of the objects in the environment.\\n\\nCome up with 10 different tasks for the robot to perform.\\n\\nGenerate the response following the template below:\\n### Task {i}: {task description}\\nwhere {i} is the task number and {task description} is the task description.\\n\\n\\nAn example task description is: {pick up the heavier block and place it on top of the lighter block}\\n\\nThe rules for task description:\\n1. Only include the objects in the environment in the task description.\\n2. The task description doesn't need to include all the objects in the environment.\\n3. You can assume the robot can do basic manipulation skills with a single parallel gripper.\\n4. The task description can be implicit in the objects. For example, {Pick up the heavier block} is a valid task description.\\n5. The task description can be implicit in the goal. For example, {Maximize the height by stacking the two blocks} is a valid task description.\\n6. Use your imagination to come up with different tasks. The tasks should be diverse and not too similar to each other.\\n7. The observation of the robot is the position and orientation of both blocks and the end effector of the robot, and the force sensor on the end effector. Try to devise tasks that are not trivial to solve with the initial observation. In other words, there are uncertainties in the task that requires the robot to explore the environment to solve the task. For tasks you think satisfy this requirement, please add a * at the end of the task description. For example, {pick up the heavier block and place it on top of the lighter block}*.\\n\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = encode_task_generation_prompt()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=message,\n",
    "    temperature=1.0,\n",
    "    top_p=1,\n",
    "    max_tokens=512,)\n",
    "response = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x7f8dd9558ef0> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"### Task 1: Pick up block B and place it directly next to block A without altering the initial orientation of block A*.\\n### Task 2: Manipulate block A and block B such that they are on the opposite sides of the table*.\\n### Task 3: Maximize the height by stacking the two blocks on top of each other*.\\n### Task 4: Pick up the lighter block and place it on the ground beneath the table without touching the heavier block.\\n### Task 5: Rotate block A 90 degrees without moving it from its initial position*.\\n### Task 6: Identify which block is denser, then lift that block and place it on top of the other block*.\\n### Task 7: Reorient blocks A and B such that they are at 180 degrees from each other with the heaviest block on the table and the lighter block on top of it*.\\n### Task 8: Move both block A and block B to the corners of the table.\\n### Task 9: Arrange blocks A and B side by side with a gap of at least 5cm between them*.\\n### Task 10: Determine which block is taller and place the shorter block on top of the taller block*.\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_task_response(response):\n",
    "    if response is None:\n",
    "        return []\n",
    "    ### Parse the response into list of tasks ###\n",
    "    raw_tasks = re.split(\"###\", response)\n",
    "    print(raw_tasks)\n",
    "    tasks = []\n",
    "    for idx, task in enumerate(raw_tasks):\n",
    "        # # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
    "        # if idx == len(raw_tasks) - 1 and response[\"finish_reason\"] == \"length\":\n",
    "        #     continue\n",
    "        ##### Parse the response into task #####\n",
    "        splitted_data = re.split(f\"{idx}:\\s+\", task)\n",
    "        if len(splitted_data) != 2:\n",
    "            continue\n",
    "        else:\n",
    "            task = splitted_data[1].strip()\n",
    "\n",
    "        ##### FILTER OUT Negative Examples #####\n",
    "        # filter out too short or too long tasks\n",
    "        if len(task.split()) <= 3 or len(task.split()) > 150:\n",
    "            continue\n",
    "        # filter based on keywords that are not suitable for language models.\n",
    "        # filter those starting with punctuation\n",
    "        if task[0] in string.punctuation:\n",
    "            continue\n",
    "        # filter those starting with non-english character\n",
    "        if not task[0].isascii():\n",
    "            continue\n",
    "        tasks.append(task)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_task_data(num_tasks_to_generate=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 10 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper. \\nThere are 2 blocks, {block A} and {block B} with randomized sizes and density in the environment.\\nThe blocks are initialized at a random position on a table.\\nContents in the {} are the names of the objects in the environment.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Methodically stack block A on top of block B regardless of their weight, in an attempt to maximize their combined height.\\'}\\n3. {\\'task\\': \"Pick and hold block A and shake it gently without dropping it to understand the nature and density of the block\\'s material.*\"}\\n4. {\\'task\\': \\'Using the force sensor, apply a specific amount of force against block B, moving it a certain distance across the table.\\'}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The i-th response need to satisfy the following format: \\n// start of instruction pair i, not including this line.\\n###\\ni.\\n<Task> {task}\\n<Instruction> {instruction}\\n<Input> {input}\\n<Output>\\n[verbal] {verbal output}\\n[action] {function output}\\n// end of instruction pair i, not including this line.\\n\\n4. The format of {instruction} will be a question. Usually it will be a question about the next action the robot should take based on the task information and robot observation.\\n5. The format of {input} will be a vector of robot observation. If there are <placeholer> in the input, it means the robot does not provide observation of this state at this step.\\n6. The format of {verbal output} will be a sentence explain the current reasoning process and the current planned action. It is used for in-context learning for multi-turn instructions.\\n7. The format of {action output} will be {function name} {function parameter}. This should be in a python executable form, don\\'t use placeholders as parameters, output the numbers if the parameters are vectors.\\n8. You should generate exact 10 instruction pairs, and make sure they are distributed among different {TASK}s.\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n###\\n###\\n###\\n###\\n###\\n <Task>: stack block A on block B\\n <Instruction>: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input>: [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output>:\\n<verbal output>Yes, the current information is enough to plan and solve the task.\\n<action output>[grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n <Task>: stack the heavier on the ligher block\\n <Instruction>: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input>: [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output>:\\n<verbal output>No. We don\\'t know the weight of the blocks, need to infer weight first.\\n<action output>[grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "prompts: 100%|██████████| 1/1 [01:24<00:00, 84.76s/it]\n",
      "\n",
      "9it [01:24,  9.42s/it]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request 0 took 84.76s\n",
      "Generated 9 instructions, kept 9 instructions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rst = generate_instruction_following_chat_data(num_instructions_to_generate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_tasks_path=\"./prompts/seeded_tasks.jsonl\"\n",
    "seed_example_path=\"./prompts/seeded_example.jsonl\"\n",
    "function_file_path=\"./prompts/skill_functions.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
    "seed_instructions = [json.loads(l) for l in open(seed_example_path, \"r\")]\n",
    "functions = [json.loads(l) for l in open(function_file_path, \"r\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = []\n",
    "for _ in range(1):\n",
    "    # only sampling from the seed tasks\n",
    "    # prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)\n",
    "    prompt = encode_instruct_prompt(\n",
    "        tasks=seed_tasks,\n",
    "        functions=functions,\n",
    "        examples=seed_instructions,\n",
    "    )\n",
    "    batch_input.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 20 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper. \\nThere are 2 blocks, {block A} and {block B} with randomized sizes and density in the environment.\\nThe blocks are initialized at a random position on a table.\\nContents in the {} are the names of the objects in the environment.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Methodically stack block A on top of block B regardless of their weight, in an attempt to maximize their combined height.\\'}\\n3. {\\'task\\': \"Pick and hold block A and shake it gently without dropping it to understand the nature and density of the block\\'s material.*\"}\\n4. {\\'task\\': \\'Using the force sensor, apply a specific amount of force against block B, moving it a certain distance across the table.\\'}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The i-th response need to satisfy the following format: \\n// start of instruction pair i, not including this line.\\n###\\ni.\\n<Task> {task}\\n<Instruction> {instruction}\\n<Input> {input}\\n<Output>\\n[verbal] {verbal output}\\n[action] {function output}\\n// end of instruction pair i, not including this line.\\n\\n4. The format of {instruction} will be a question. Usually it will be a question about the next action the robot should take based on the task information and robot observation.\\n5. The format of {input} will be a vector of robot observation. If there are <placeholer> in the input, it means the robot does not provide observation of this state at this step.\\n6. The format of {verbal output} will be a sentence explain the current reasoning process and the current planned action. It is used for in-context learning for multi-turn instructions.\\n7. The format of {action output} will be {function name} {function parameter}. This should be in a python executable form, don\\'t use placeholders as parameters, output the numbers if the parameters are vectors.\\n8. You should generate exact 20 instruction pairs, and make sure they are distributed among different {TASK}s.\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n###\\n###\\n###\\n###\\n###\\n <Task>: stack block A on block B\\n <Instruction>: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input>: [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output>:\\n<verbal output>Yes, the current information is enough to plan and solve the task.\\n<action output>[grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n <Task>: stack the heavier on the ligher block\\n <Instruction>: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input>: [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output>:\\n<verbal output>No. We don\\'t know the weight of the blocks, need to infer weight first.\\n<action output>[grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [02:14<00:00, 134.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "decoding_args = utils.OpenAIChatDecodingArguments(\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=2048,\n",
    "    stop=[\"\\n21\", \"21.\"],\n",
    ")\n",
    "chatcompletions = utils.openai_chatcompletion(\n",
    "    prompts=batch_input,\n",
    "    model_name=\"gpt-4\",\n",
    "    decoding_args=decoding_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "for completion in chatcompletions:\n",
    "    new_instructions = post_process_chat_response(completion)\n",
    "    instructions += new_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9\n",
      "['', '[verbal]', ' Based on the density, block A is lighter. The strategy will be to pick up block A and then place it at the position [1.75, 2.75, 0.75].\\n', '[action]', ' reach([1.12, 1.45, 0.75]), grasp(blockA), place(blockA, [1.75, 2.75, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' Yes, the robot should be able to reach block A.\\n', '[action]', ' reach([1.12, 1.45, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' The robot should reach towards block A, grasp it, bring it over block B, then place it on top.\\n', '[action]', ' reach([2, 4, 0.75]), grasp(blockA), place(blockA, [2.5, 3.75, 1.2])']\n",
      "9\n",
      "['', '[verbal]', ' The robot should reach towards block A, grasp it and then shake it. Shaking will be simulated by moving the grasped block back and forth.\\n', '[action]', ' reach([1.75, 2.5, 0.45]), grasp(blockA), move_to([1.75, 2.5, 0.65]), move_to([1.75, 2.5, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' The lighter block, which is Block A, should be placed at the position [1.75, 2.75, 0.75]. \\n', '[action]', ' place(blockA, [1.75, 2.75, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' No, in this case you should reach for block A first as we are stacking it on top of block B.\\n', '[action]', ' reach([2, 4, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' Block A should be placed at the position [2.5, 3.75, 1.2] to stack it on top of block B.\\n', '[action]', ' place(blockA, [2.5, 3.75, 1.2])']\n",
      "9\n",
      "['', '[verbal]', ' You can reach the block A from its current position which is [1.75, 2.5, 0.45].\\n', '[action]', ' reach([1.75, 2.5, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' To apply force to block B, robot should reach to it, grasp it, and move to the desired position.\\n', '[action]', ' reach([2.5, 3.75, 0.45]), grasp(blockB), move_to([3, 3.75, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' After applying force, block B can be moved to the position [3, 3.75, 0.45].\\n', '[action]', ' move_to([3, 3.75, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' With respect to the provided densities, block A seems to be lighter.\\n', '[action]', ' <noaction>']\n",
      "9\n",
      "['', '[verbal]', ' Yes, we can stack block A on block B without knowing their weights.\\n', '[action]', ' reach([2, 4, 0.75]), grasp(blockA), place(blockA, [2.5, 3.75, 1.2])']\n",
      "9\n",
      "['', '[verbal]', ' As there is no time parameter in the task, the decision will be arbitrary, for example, a two-step shake.\\n', '[action]', ' move_to([1.75, 2.5, 0.65]), move_to([1.75, 2.5, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' You can use your gripper to apply force and push block B across the table.\\n', '[action]', ' reach([2.5, 3.75, 0.45]), grasp(blockB), move_to([3, 3.75, 0.45])']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "response = completion\n",
    "raw_instructions = response[\"message\"][\"content\"]\n",
    "raw_instructions = re.split(\"###\", raw_instructions)\n",
    "\n",
    "for idx, inst in enumerate(raw_instructions):\n",
    "    # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
    "    if idx == len(raw_instructions) - 1 and response[\"finish_reason\"] == \"length\":\n",
    "        continue\n",
    "    ##### Parse the response into instruction, input, output #####\n",
    "    idx += 1\n",
    "    splitted_data = re.split(f\"(<Task>|<Instruction>|<Input>|<Output>)\", inst)\n",
    "    print(len(splitted_data))\n",
    "    if len(splitted_data) != 9:\n",
    "        continue\n",
    "    else:\n",
    "        task = splitted_data[2].strip()\n",
    "        inst = splitted_data[4].strip()\n",
    "        input = splitted_data[6].strip()\n",
    "        input = \"\" if input.lower() == \"<noinput>\" else input\n",
    "        output = splitted_data[8].strip()\n",
    "        # parse output into <verbal output> and <action output>\n",
    "        output_splitted_data = re.split(\n",
    "            f\"(\\[verbal\\]|\\[action\\])\", output\n",
    "        )\n",
    "        print(output_splitted_data)\n",
    "        verbal_output = output_splitted_data[2].strip()\n",
    "        action_output = output_splitted_data[4].strip()\n",
    "        ##### FILTER OUT Negative Examples #####\n",
    "        # filter out too short or too long instructions\n",
    "        if len(inst.split()) <= 3 or len(inst.split()) > 150:\n",
    "            continue\n",
    "        # filter based on keywords that are not suitable for language models.\n",
    "        # filter those starting with punctuation\n",
    "        if inst[0] in string.punctuation:\n",
    "            continue\n",
    "        # filter those starting with non-english character\n",
    "        if not inst[0].isascii():\n",
    "            continue\n",
    "        instructions.append(\n",
    "            {\n",
    "                \"task\": task,\n",
    "                \"instruction\": inst,\n",
    "                \"input\": input,\n",
    "                \"verbal_output\": verbal_output,\n",
    "                \"action_output\": action_output,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Based on the initial observation, which is the lighter block and what is the strategy to move it to the opposite end of the table?',\n",
       " 'input': '[[1.12, 1.45, 0.75], [1.75, 2.75, 0.45], [0.0, 0.0, 3]]',\n",
       " 'verbal_output': 'Based on the density, block A is lighter. The strategy will be to pick up block A and then place it at the position [1.75, 2.75, 0.75].',\n",
       " 'action_output': 'reach([1.12, 1.45, 0.75]), grasp(blockA), place(blockA, [1.75, 2.75, 0.75])'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "201e8860b8e6658cdfe301dc13e2a8b5ecdc17e6f5f25d00e326e3c8c2d13630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
