{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_robot_instruction import encode_task_generation_prompt, generate_task_data, encode_instruct_prompt, generate_instruction_following_chat_data, post_process_chat_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"You are serving as a task-generation helper for a given robot environment.\\xa0\\n\\nHere is the information about the environment. The environment is called {Two Block World}\\xa0\\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe blocks are initialized at a random position on a table.\\nContents in the {} are the names of the objects in the environment.\\n\\nCome up with 10 different tasks for the robot to perform.\\n\\nGenerate the response following the template below:\\n### Task {i}: {task description}\\nwhere {i} is the task number and {task description} is the task description.\\n\\n\\nAn example task description is: {pick up the heavier block and place it on top of the lighter block}\\n\\nThe rules for task description:\\n1. Only include the objects in the environment in the task description.\\n2. The task description doesn't need to include all the objects in the environment.\\n3. You can assume the robot can do basic manipulation skills with a single parallel gripper.\\n4. The task description can be implicit in the objects. For example, {Pick up the heavier block} is a valid task description.\\n5. The task description can be implicit in the goal. For example, {Maximize the height by stacking the two blocks} is a valid task description.\\n6. Use your imagination to come up with different tasks. The tasks should be diverse and not too similar to each other.\\n7. The observation of the robot is the position and orientation of both blocks and the end effector of the robot, and the force sensor on the end effector. Try to devise tasks that are not trivial to solve with the initial observation. In other words, there are uncertainties in the task that requires the robot to explore the environment to solve the task. For tasks you think satisfy this requirement, please add a * at the end of the task description. For example, {pick up the heavier block and place it on top of the lighter block}*.\\n\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = encode_task_generation_prompt()\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=message,\n",
    "    temperature=1.0,\n",
    "    top_p=1,\n",
    "    max_tokens=512,)\n",
    "response = output['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x7f8dd9558ef0> JSON: {\n",
       "   \"index\": 0,\n",
       "   \"message\": {\n",
       "     \"role\": \"assistant\",\n",
       "     \"content\": \"### Task 1: Pick up block B and place it directly next to block A without altering the initial orientation of block A*.\\n### Task 2: Manipulate block A and block B such that they are on the opposite sides of the table*.\\n### Task 3: Maximize the height by stacking the two blocks on top of each other*.\\n### Task 4: Pick up the lighter block and place it on the ground beneath the table without touching the heavier block.\\n### Task 5: Rotate block A 90 degrees without moving it from its initial position*.\\n### Task 6: Identify which block is denser, then lift that block and place it on top of the other block*.\\n### Task 7: Reorient blocks A and B such that they are at 180 degrees from each other with the heaviest block on the table and the lighter block on top of it*.\\n### Task 8: Move both block A and block B to the corners of the table.\\n### Task 9: Arrange blocks A and B side by side with a gap of at least 5cm between them*.\\n### Task 10: Determine which block is taller and place the shorter block on top of the taller block*.\"\n",
       "   },\n",
       "   \"finish_reason\": \"stop\"\n",
       " }]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_task_response(response):\n",
    "    if response is None:\n",
    "        return []\n",
    "    ### Parse the response into list of tasks ###\n",
    "    raw_tasks = re.split(\"###\", response)\n",
    "    print(raw_tasks)\n",
    "    tasks = []\n",
    "    for idx, task in enumerate(raw_tasks):\n",
    "        # # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
    "        # if idx == len(raw_tasks) - 1 and response[\"finish_reason\"] == \"length\":\n",
    "        #     continue\n",
    "        ##### Parse the response into task #####\n",
    "        splitted_data = re.split(f\"{idx}:\\s+\", task)\n",
    "        if len(splitted_data) != 2:\n",
    "            continue\n",
    "        else:\n",
    "            task = splitted_data[1].strip()\n",
    "\n",
    "        ##### FILTER OUT Negative Examples #####\n",
    "        # filter out too short or too long tasks\n",
    "        if len(task.split()) <= 3 or len(task.split()) > 150:\n",
    "            continue\n",
    "        # filter based on keywords that are not suitable for language models.\n",
    "        # filter those starting with punctuation\n",
    "        if task[0] in string.punctuation:\n",
    "            continue\n",
    "        # filter those starting with non-english character\n",
    "        if not task[0].isascii():\n",
    "            continue\n",
    "        tasks.append(task)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 machine-generated tasks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [[{'role': 'user', 'content': \"You are serving as a task-generation helper for a given robot environment.\\xa0\\n\\nHere is the information about the environment. The environment is called {Two Block World}\\xa0\\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe Franka robot has a force sensor on the end effector.\\nThe Franka robot is mounted on a table.\\nThe blocks are initialized at a random position on the table.\\nContents in the {} are the names of the objects in the environment.\\n\\nCome up with 10 different tasks for the robot to perform.\\n\\nGenerate the response following the template below:\\n###Task {i}: {task description}\\nwhere {i} is the task number and {task description} is the task description.\\n\\n\\nAn example task description is: {pick up the heavier block and place it on top of the lighter block}\\n\\nThe rules for task description:\\n1. Only include the objects in the environment in the task description.\\n2. The task description doesn't need to include all the objects in the environment.\\n3. You can assume the robot can do basic manipulation skills with a single parallel gripper.\\n4. The task description can be implicit in the objects. For example, {Pick up the heavier block} is a valid task description.\\n5. The task description can be implicit in the goal. For example, {Maximize the height by stacking the two blocks} is a valid task description.\\n6. Use your imagination to come up with different tasks. The tasks should be diverse and not too similar to each other.\\n7. The observation of the robot is the position and orientation of both blocks and the end effector of the robot, and the force sensor on the end effector. Try to devise tasks that are not trivial to solve with the initial observation. In other words, there are uncertainties in the task that requires the robot to explore the environment to solve the task. For tasks you think satisfy this requirement, please add a * at the end of the task description. For example, {pick up the heavier block and place it on top of the lighter block}*.\\n\"}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [00:14<00:00, 14.07s/it]\n",
      "  2%|▏         | 1/50 [00:14<11:29, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Task 1: Stack block A on top of block B without inducing a net force change exceeding 5N on the force sensor.*\\n', 'Task 2: Repeatedly hand over block A from the left gripper to the right gripper without dropping it.\\n', 'Task 3: Move block B to the edge of the table and bring it back to the center without tipping it over.*\\n', 'Task 4: Exchange the positions of the two blocks without touching the table. \\n', 'Task 5: Arrange block B in a position relative to block A, where both are balanced on opposing corners.*\\n', 'Task 6: Rotate block A 180 degrees around its center axis without changing its position on the table.*\\n', 'Task 7: Determine the heavier block by lifting each block by a certain height. Then place the lighter block on top of the heavier one*.\\n', 'Task 8: Test the robustness of the gripper by applying varying loadings using block A and recording the force on the force sensor.*\\n', 'Task 9: Position block B such that its center of mass is directly over the center of block A.*\\n', 'Task 10: Determine the most stable orientation for block A by placing it at different orientations and recording the force on the force sensor.*']\n",
      "Request 1 took 14.08s\n",
      "Generated 10 tasks, kept 10 instructions\n",
      "### [[{'role': 'user', 'content': \"You are serving as a task-generation helper for a given robot environment.\\xa0\\n\\nHere is the information about the environment. The environment is called {Two Block World}\\xa0\\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe Franka robot has a force sensor on the end effector.\\nThe Franka robot is mounted on a table.\\nThe blocks are initialized at a random position on the table.\\nContents in the {} are the names of the objects in the environment.\\n\\nCome up with 10 different tasks for the robot to perform.\\n\\nGenerate the response following the template below:\\n###Task {i}: {task description}\\nwhere {i} is the task number and {task description} is the task description.\\n\\n\\nAn example task description is: {pick up the heavier block and place it on top of the lighter block}\\n\\nThe rules for task description:\\n1. Only include the objects in the environment in the task description.\\n2. The task description doesn't need to include all the objects in the environment.\\n3. You can assume the robot can do basic manipulation skills with a single parallel gripper.\\n4. The task description can be implicit in the objects. For example, {Pick up the heavier block} is a valid task description.\\n5. The task description can be implicit in the goal. For example, {Maximize the height by stacking the two blocks} is a valid task description.\\n6. Use your imagination to come up with different tasks. The tasks should be diverse and not too similar to each other.\\n7. The observation of the robot is the position and orientation of both blocks and the end effector of the robot, and the force sensor on the end effector. Try to devise tasks that are not trivial to solve with the initial observation. In other words, there are uncertainties in the task that requires the robot to explore the environment to solve the task. For tasks you think satisfy this requirement, please add a * at the end of the task description. For example, {pick up the heavier block and place it on top of the lighter block}*.\\n\"}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [00:12<00:00, 12.86s/it]\n",
      " 22%|██▏       | 11/50 [00:26<01:22,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', ' Task 1: Stack block A on top of block B regardless of their weight.\\n', ' Task 2: Drop the heavier block on top of the lighter one such that the lighter block sustains minimum damage.*\\n', ' Task 3: Move block B to the edge of the table without letting it fall.*\\n', ' Task 4: Position the two blocks next to each other in such a way that they touch each other from the largest flat surfaces.*\\n', ' Task 5: Arrange the blocks in ascending order of their mass starting from the left side of the table.*\\n', ' Task 6: Flip block A upside down without touching block B.*\\n', ' Task 7: Stack block B over block A only if block B is lighter.*\\n', ' Task 8: Move block A to the right-most corner of the table.*\\n', \" Task 9: Align the two blocks by one of their sides making sure they don't overlap.*\\n\", ' Task 10: Maximize the distance between the two blocks on the table.*']\n",
      "Request 2 took 12.87s\n",
      "Generated 10 tasks, kept 10 instructions\n",
      "### [[{'role': 'user', 'content': \"You are serving as a task-generation helper for a given robot environment.\\xa0\\n\\nHere is the information about the environment. The environment is called {Two Block World}\\xa0\\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe Franka robot has a force sensor on the end effector.\\nThe Franka robot is mounted on a table.\\nThe blocks are initialized at a random position on the table.\\nContents in the {} are the names of the objects in the environment.\\n\\nCome up with 10 different tasks for the robot to perform.\\n\\nGenerate the response following the template below:\\n###Task {i}: {task description}\\nwhere {i} is the task number and {task description} is the task description.\\n\\n\\nAn example task description is: {pick up the heavier block and place it on top of the lighter block}\\n\\nThe rules for task description:\\n1. Only include the objects in the environment in the task description.\\n2. The task description doesn't need to include all the objects in the environment.\\n3. You can assume the robot can do basic manipulation skills with a single parallel gripper.\\n4. The task description can be implicit in the objects. For example, {Pick up the heavier block} is a valid task description.\\n5. The task description can be implicit in the goal. For example, {Maximize the height by stacking the two blocks} is a valid task description.\\n6. Use your imagination to come up with different tasks. The tasks should be diverse and not too similar to each other.\\n7. The observation of the robot is the position and orientation of both blocks and the end effector of the robot, and the force sensor on the end effector. Try to devise tasks that are not trivial to solve with the initial observation. In other words, there are uncertainties in the task that requires the robot to explore the environment to solve the task. For tasks you think satisfy this requirement, please add a * at the end of the task description. For example, {pick up the heavier block and place it on top of the lighter block}*.\\n\"}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [00:17<00:00, 17.06s/it]\n",
      " 60%|██████    | 30/50 [00:44<00:29,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'Task 1: Pick up block A and place it on block B without tipping over block B*.\\n', 'Task 2: Arrange block A and block B side by side on the table with block A on the right and block B on the left*.\\n', 'Task 3: Determine which block is denser based by lifting each block and comparing the force sensor readings*.\\n', 'Task 4: Stack block A directly on top of block B without tilting either block*.\\n', 'Task 5: Reorder the blocks from left to right on the table based on their weight, with the heavier block on the left*.\\n', 'Task 6: Pick up and then release block B from a predefined height to simulate a drop test*.\\n', 'Task 7: Rotate block B 90 degrees clockwise while keeping it in its initial position*.\\n', 'Task 8: Move block A to the leftmost area of the table and block B to the rightmost area without changing their initial orientations*.\\n', 'Task 9: Balance block B on top of block A, without any part of block B hanging over the edge of block A*.\\n', \"Task 10: Move block A to the front right corner of the table and block B to the rear left corner, considering the skewed view from the Franka robot's placement*.\"]\n",
      "Request 3 took 17.07s\n",
      "Generated 10 tasks, kept 10 instructions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_task_data(num_tasks_to_generate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 624 machine-generated instructions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 20 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe Franka robot has a force sensor on the end effector.\\nThe Franka robot is mounted on a table.\\nThe blocks are initialized at a random position on the table.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\n{TRAJECTORY_PLACEHOLDER}\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Drop the heavier block on top of the lighter one such that the lighter block sustains minimum damage.*\\'}\\n3. {\\'task\\': \\'Move block B to the edge of the table without letting it fall.*\\'}\\n4. {\\'task\\': \\'Position the two blocks next to each other in such a way that they touch each other from the largest flat surfaces.*\\'}\\n5. {\\'task\\': \\'Arrange the blocks in ascending order of their mass starting from the left side of the table.*\\'}\\n6. {\\'task\\': \\'Flip block A upside down without touching block B.*\\'}\\n7. {\\'task\\': \\'Stack block B over block A only if block B is lighter.*\\'}\\n8. {\\'task\\': \\'Move block A to the right-most corner of the table.*\\'}\\n9. {\\'task\\': \"Align the two blocks by one of their sides making sure they don\\'t overlap.*\"}\\n10. {\\'task\\': \\'Maximize the distance between the two blocks on the table.*\\'}\\n11. {\\'task\\': \\'Pick up block A and place it on block B without tipping over block B*.\\'}\\n12. {\\'task\\': \\'Arrange block A and block B side by side on the table with block A on the right and block B on the left*.\\'}\\n13. {\\'task\\': \\'Determine which block is denser based by lifting each block and comparing the force sensor readings*.\\'}\\n14. {\\'task\\': \\'Stack block A directly on top of block B without tilting either block*.\\'}\\n15. {\\'task\\': \\'Reorder the blocks from left to right on the table based on their weight, with the heavier block on the left*.\\'}\\n16. {\\'task\\': \\'Pick up and then release block B from a predefined height to simulate a drop test*.\\'}\\n17. {\\'task\\': \\'Rotate block B 90 degrees clockwise while keeping it in its initial position*.\\'}\\n18. {\\'task\\': \\'Move block A to the leftmost area of the table and block B to the rightmost area without changing their initial orientations*.\\'}\\n19. {\\'task\\': \\'Balance block B on top of block A, without any part of block B hanging over the edge of block A*.\\'}\\n20. {\\'task\\': \"Move block A to the front right corner of the table and block B to the rear left corner, considering the skewed view from the Franka robot\\'s placement*.\"}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The i-th response need to satisfy the following format: \\n// start of instruction pair i, not including this line.\\n###\\ni.\\n<Task> {task}\\n<Instruction> {instruction}\\n<Input> {input}\\n<Output>\\n[verbal] {verbal output}\\n[action] {list of function output}\\n// end of instruction pair i, not including this line.\\n\\n4. The format of {instruction} will be a question. Usually it will be a question about the next action the robot should take based on the task information and robot observation. It can also input the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat. \\n5. The format of {input} will be a vector of robot observation. The input should involve all the states in the robot obvervation. There might be <placeholer> in the examples\\' input, but replace them with actual numbers in the generated instruction pairs.\\n6. The format of {verbal output} will be a sentence explain the current reasoning process and the current planned action. It is used for in-context learning for multi-turn instructions.\\n7. The format of {action output} will be list of {function name} {function parameter} wrapped by []. Each element should be in a python executable form, don\\'t use placeholders as parameters, output the numbers if the parameters are vectors.\\n8. You should generate exact 20 instruction pairs, and make sure they are distributed among different {TASK}s.\\n9. Each instruction pair should be separated by a line of \"###\" at the beginning.\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n <Task> stack block A on block B\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] Yes, the current information is enough to plan and solve the task.\\n[action] [grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n <Task> stack the heavier on the ligher block\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] No. We don\\'t know the weight of the blocks, need to infer weight first.\\n[action][grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [02:11<00:00, 131.08s/it]\n",
      " 62%|██████▎   | 625/1000 [02:11<01:18,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 634/1000 [02:11<01:16,  4.77it/s]\n",
      "Request 1 took 131.09s\n",
      "Generated 10 instructions, kept 10 instructions\n",
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 20 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe Franka robot has a force sensor on the end effector.\\nThe Franka robot is mounted on a table.\\nThe blocks are initialized at a random position on the table.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\n{TRAJECTORY_PLACEHOLDER}\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Drop the heavier block on top of the lighter one such that the lighter block sustains minimum damage.*\\'}\\n3. {\\'task\\': \\'Move block B to the edge of the table without letting it fall.*\\'}\\n4. {\\'task\\': \\'Position the two blocks next to each other in such a way that they touch each other from the largest flat surfaces.*\\'}\\n5. {\\'task\\': \\'Arrange the blocks in ascending order of their mass starting from the left side of the table.*\\'}\\n6. {\\'task\\': \\'Flip block A upside down without touching block B.*\\'}\\n7. {\\'task\\': \\'Stack block B over block A only if block B is lighter.*\\'}\\n8. {\\'task\\': \\'Move block A to the right-most corner of the table.*\\'}\\n9. {\\'task\\': \"Align the two blocks by one of their sides making sure they don\\'t overlap.*\"}\\n10. {\\'task\\': \\'Maximize the distance between the two blocks on the table.*\\'}\\n11. {\\'task\\': \\'Pick up block A and place it on block B without tipping over block B*.\\'}\\n12. {\\'task\\': \\'Arrange block A and block B side by side on the table with block A on the right and block B on the left*.\\'}\\n13. {\\'task\\': \\'Determine which block is denser based by lifting each block and comparing the force sensor readings*.\\'}\\n14. {\\'task\\': \\'Stack block A directly on top of block B without tilting either block*.\\'}\\n15. {\\'task\\': \\'Reorder the blocks from left to right on the table based on their weight, with the heavier block on the left*.\\'}\\n16. {\\'task\\': \\'Pick up and then release block B from a predefined height to simulate a drop test*.\\'}\\n17. {\\'task\\': \\'Rotate block B 90 degrees clockwise while keeping it in its initial position*.\\'}\\n18. {\\'task\\': \\'Move block A to the leftmost area of the table and block B to the rightmost area without changing their initial orientations*.\\'}\\n19. {\\'task\\': \\'Balance block B on top of block A, without any part of block B hanging over the edge of block A*.\\'}\\n20. {\\'task\\': \"Move block A to the front right corner of the table and block B to the rear left corner, considering the skewed view from the Franka robot\\'s placement*.\"}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The i-th response need to satisfy the following format: \\n// start of instruction pair i, not including this line.\\n###\\ni.\\n<Task> {task}\\n<Instruction> {instruction}\\n<Input> {input}\\n<Output>\\n[verbal] {verbal output}\\n[action] {list of function output}\\n// end of instruction pair i, not including this line.\\n\\n4. The format of {instruction} will be a question. Usually it will be a question about the next action the robot should take based on the task information and robot observation. It can also input the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat. \\n5. The format of {input} will be a vector of robot observation. The input should involve all the states in the robot obvervation. There might be <placeholer> in the examples\\' input, but replace them with actual numbers in the generated instruction pairs.\\n6. The format of {verbal output} will be a sentence explain the current reasoning process and the current planned action. It is used for in-context learning for multi-turn instructions.\\n7. The format of {action output} will be list of {function name} {function parameter} wrapped by []. Each element should be in a python executable form, don\\'t use placeholders as parameters, output the numbers if the parameters are vectors.\\n8. You should generate exact 20 instruction pairs, and make sure they are distributed among different {TASK}s.\\n9. Each instruction pair should be separated by a line of \"###\" at the beginning.\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n <Task> stack block A on block B\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] Yes, the current information is enough to plan and solve the task.\\n[action] [grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n <Task> stack the heavier on the ligher block\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] No. We don\\'t know the weight of the blocks, need to infer weight first.\\n[action][grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [01:14<00:00, 74.49s/it]\n",
      " 64%|██████▎   | 635/1000 [03:25<02:15,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 639/1000 [03:25<02:14,  2.69it/s]\n",
      "Request 2 took 74.49s\n",
      "Generated 5 instructions, kept 5 instructions\n",
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 20 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe Franka robot has a force sensor on the end effector.\\nThe Franka robot is mounted on a table.\\nThe blocks are initialized at a random position on the table.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\n{TRAJECTORY_PLACEHOLDER}\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Drop the heavier block on top of the lighter one such that the lighter block sustains minimum damage.*\\'}\\n3. {\\'task\\': \\'Move block B to the edge of the table without letting it fall.*\\'}\\n4. {\\'task\\': \\'Position the two blocks next to each other in such a way that they touch each other from the largest flat surfaces.*\\'}\\n5. {\\'task\\': \\'Arrange the blocks in ascending order of their mass starting from the left side of the table.*\\'}\\n6. {\\'task\\': \\'Flip block A upside down without touching block B.*\\'}\\n7. {\\'task\\': \\'Stack block B over block A only if block B is lighter.*\\'}\\n8. {\\'task\\': \\'Move block A to the right-most corner of the table.*\\'}\\n9. {\\'task\\': \"Align the two blocks by one of their sides making sure they don\\'t overlap.*\"}\\n10. {\\'task\\': \\'Maximize the distance between the two blocks on the table.*\\'}\\n11. {\\'task\\': \\'Pick up block A and place it on block B without tipping over block B*.\\'}\\n12. {\\'task\\': \\'Arrange block A and block B side by side on the table with block A on the right and block B on the left*.\\'}\\n13. {\\'task\\': \\'Determine which block is denser based by lifting each block and comparing the force sensor readings*.\\'}\\n14. {\\'task\\': \\'Stack block A directly on top of block B without tilting either block*.\\'}\\n15. {\\'task\\': \\'Reorder the blocks from left to right on the table based on their weight, with the heavier block on the left*.\\'}\\n16. {\\'task\\': \\'Pick up and then release block B from a predefined height to simulate a drop test*.\\'}\\n17. {\\'task\\': \\'Rotate block B 90 degrees clockwise while keeping it in its initial position*.\\'}\\n18. {\\'task\\': \\'Move block A to the leftmost area of the table and block B to the rightmost area without changing their initial orientations*.\\'}\\n19. {\\'task\\': \\'Balance block B on top of block A, without any part of block B hanging over the edge of block A*.\\'}\\n20. {\\'task\\': \"Move block A to the front right corner of the table and block B to the rear left corner, considering the skewed view from the Franka robot\\'s placement*.\"}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The i-th response need to satisfy the following format: \\n// start of instruction pair i, not including this line.\\n###\\ni.\\n<Task> {task}\\n<Instruction> {instruction}\\n<Input> {input}\\n<Output>\\n[verbal] {verbal output}\\n[action] {list of function output}\\n// end of instruction pair i, not including this line.\\n\\n4. The format of {instruction} will be a question. Usually it will be a question about the next action the robot should take based on the task information and robot observation. It can also input the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat. \\n5. The format of {input} will be a vector of robot observation. The input should involve all the states in the robot obvervation. There might be <placeholer> in the examples\\' input, but replace them with actual numbers in the generated instruction pairs.\\n6. The format of {verbal output} will be a sentence explain the current reasoning process and the current planned action. It is used for in-context learning for multi-turn instructions.\\n7. The format of {action output} will be list of {function name} {function parameter} wrapped by []. Each element should be in a python executable form, don\\'t use placeholders as parameters, output the numbers if the parameters are vectors.\\n8. You should generate exact 20 instruction pairs, and make sure they are distributed among different {TASK}s.\\n9. Each instruction pair should be separated by a line of \"###\" at the beginning.\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n <Task> stack block A on block B\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] Yes, the current information is enough to plan and solve the task.\\n[action] [grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n <Task> stack the heavier on the ligher block\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] No. We don\\'t know the weight of the blocks, need to infer weight first.\\n[action][grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [04:17<00:00, 257.25s/it]\n",
      " 64%|██████▍   | 640/1000 [07:42<07:02,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 659/1000 [07:42<06:40,  1.17s/it]\n",
      "Request 3 took 257.26s\n",
      "Generated 20 instructions, kept 20 instructions\n",
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 20 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper.\\xa0\\nThere are two blocks, {block A} and {block B}, with randomized sizes and density in the environment.\\nThe Franka robot has a force sensor on the end effector.\\nThe Franka robot is mounted on a table.\\nThe blocks are initialized at a random position on the table.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\n{TRAJECTORY_PLACEHOLDER}\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Drop the heavier block on top of the lighter one such that the lighter block sustains minimum damage.*\\'}\\n3. {\\'task\\': \\'Move block B to the edge of the table without letting it fall.*\\'}\\n4. {\\'task\\': \\'Position the two blocks next to each other in such a way that they touch each other from the largest flat surfaces.*\\'}\\n5. {\\'task\\': \\'Arrange the blocks in ascending order of their mass starting from the left side of the table.*\\'}\\n6. {\\'task\\': \\'Flip block A upside down without touching block B.*\\'}\\n7. {\\'task\\': \\'Stack block B over block A only if block B is lighter.*\\'}\\n8. {\\'task\\': \\'Move block A to the right-most corner of the table.*\\'}\\n9. {\\'task\\': \"Align the two blocks by one of their sides making sure they don\\'t overlap.*\"}\\n10. {\\'task\\': \\'Maximize the distance between the two blocks on the table.*\\'}\\n11. {\\'task\\': \\'Pick up block A and place it on block B without tipping over block B*.\\'}\\n12. {\\'task\\': \\'Arrange block A and block B side by side on the table with block A on the right and block B on the left*.\\'}\\n13. {\\'task\\': \\'Determine which block is denser based by lifting each block and comparing the force sensor readings*.\\'}\\n14. {\\'task\\': \\'Stack block A directly on top of block B without tilting either block*.\\'}\\n15. {\\'task\\': \\'Reorder the blocks from left to right on the table based on their weight, with the heavier block on the left*.\\'}\\n16. {\\'task\\': \\'Pick up and then release block B from a predefined height to simulate a drop test*.\\'}\\n17. {\\'task\\': \\'Rotate block B 90 degrees clockwise while keeping it in its initial position*.\\'}\\n18. {\\'task\\': \\'Move block A to the leftmost area of the table and block B to the rightmost area without changing their initial orientations*.\\'}\\n19. {\\'task\\': \\'Balance block B on top of block A, without any part of block B hanging over the edge of block A*.\\'}\\n20. {\\'task\\': \"Move block A to the front right corner of the table and block B to the rear left corner, considering the skewed view from the Franka robot\\'s placement*.\"}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The i-th response need to satisfy the following format: \\n// start of instruction pair i, not including this line.\\n###\\ni.\\n<Task> {task}\\n<Instruction> {instruction}\\n<Input> {input}\\n<Output>\\n[verbal] {verbal output}\\n[action] {list of function output}\\n// end of instruction pair i, not including this line.\\n\\n4. The format of {instruction} will be a question. Usually it will be a question about the next action the robot should take based on the task information and robot observation. It can also input the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat. \\n5. The format of {input} will be a vector of robot observation. The input should involve all the states in the robot obvervation. There might be <placeholer> in the examples\\' input, but replace them with actual numbers in the generated instruction pairs.\\n6. The format of {verbal output} will be a sentence explain the current reasoning process and the current planned action. It is used for in-context learning for multi-turn instructions.\\n7. The format of {action output} will be list of {function name} {function parameter} wrapped by []. Each element should be in a python executable form, don\\'t use placeholders as parameters, output the numbers if the parameters are vectors.\\n8. You should generate exact 20 instruction pairs, and make sure they are distributed among different {TASK}s.\\n9. Each instruction pair should be separated by a line of \"###\" at the beginning.\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n <Task> stack block A on block B\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] Yes, the current information is enough to plan and solve the task.\\n[action] [grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n <Task> stack the heavier on the ligher block\\n <Instruction> Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input> [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output> \\n[verbal] No. We don\\'t know the weight of the blocks, need to infer weight first.\\n[action][grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts:   0%|          | 0/1 [00:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ret \u001b[39m=\u001b[39m generate_instruction_following_chat_data(num_instructions_to_generate\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[0;32m~/codebase/alpaca4robotics/generate_robot_instruction.py:315\u001b[0m, in \u001b[0;36mgenerate_instruction_following_chat_data\u001b[0;34m(output_dir, seed_tasks_path, seed_example_path, function_file_path, subskill_data_path, num_instructions_to_generate, model_name, num_prompt_instructions, request_batch_size, temperature, top_p, num_cpus)\u001b[0m\n\u001b[1;32m    308\u001b[0m decoding_args \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mOpenAIChatDecodingArguments(\n\u001b[1;32m    309\u001b[0m     temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    310\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    311\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m4096\u001b[39m,\n\u001b[1;32m    312\u001b[0m     stop\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m21\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m21.\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    313\u001b[0m )\n\u001b[1;32m    314\u001b[0m request_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 315\u001b[0m chatcompletions \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mopenai_chatcompletion(\n\u001b[1;32m    316\u001b[0m     prompts\u001b[39m=\u001b[39;49mbatch_input,\n\u001b[1;32m    317\u001b[0m     model_name\u001b[39m=\u001b[39;49mmodel_name,\n\u001b[1;32m    318\u001b[0m     decoding_args\u001b[39m=\u001b[39;49mdecoding_args,\n\u001b[1;32m    319\u001b[0m )\n\u001b[1;32m    320\u001b[0m request_duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m request_start_time\n\u001b[1;32m    321\u001b[0m instructions \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/codebase/alpaca4robotics/utils.py:196\u001b[0m, in \u001b[0;36mopenai_chatcompletion\u001b[0;34m(prompts, decoding_args, model_name, sleep_time, max_instances, return_text, **decoding_kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     shared_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    192\u001b[0m         model\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m    193\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch_decoding_args\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m,\n\u001b[1;32m    194\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecoding_kwargs,\n\u001b[1;32m    195\u001b[0m     )\n\u001b[0;32m--> 196\u001b[0m     chat_response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    197\u001b[0m         messages\u001b[39m=\u001b[39;49mprompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mshared_kwargs)\n\u001b[1;32m    198\u001b[0m     choices \u001b[39m=\u001b[39m chat_response\u001b[39m.\u001b[39mchoices\n\u001b[1;32m    200\u001b[0m     \u001b[39mfor\u001b[39;00m choice \u001b[39min\u001b[39;00m choices:\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/site-packages/urllib3/connection.py:454\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    453\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    456\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    457\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/alpaca/lib/python3.10/ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ret = generate_instruction_following_chat_data(num_instructions_to_generate=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.json2jsonl(\"gpt4_generation/instruct_regen.json\", \"gpt4_generation/instruct_regen.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n",
      "['1.\\n', '<Task>', ' Identify and pick up the lighter block and move it to the opposite end of the table.\\n', '<Instruction>', ' According to the current situation, should the robot first determine the weight of the blocks to identify the lighter one?\\n', '<Input>', ' [[0.50, 0.30, 1.025], [<placeholder>], [0.60, 0.50, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] Yes, the robot needs to first determine the weight of the blocks to identify which one is lighter.\\n[action] [grasp(\"blockA\"), grasp(\"blockB\")]\\n\\n2.\\n', '<Task>', ' Drop the heavier block on top of the lighter one such that the lighter block sustains minimum damage.\\n', '<Instruction>', ' Given the last processed action, which block is heavier and should it be moved on top of the lighter one in a careful manner to prevent causing damage?\\n', '<Input>', ' <noinput>\\n', '<Output>', '\\n[verbal] Block B is heavier. After identifying, we should move it carefully on top of the lighter block, Block A.\\n[action] [grasp(\"blockB\"), place(\"blockB\", [0.50, 0.30, 1.15])]\\n\\n3.\\n', '<Task>', ' Move block B to the edge of the table without letting it fall.\\n', '<Instruction>', ' Considering the specified task, what should be the first action to strategically move block B to the edge of the table without making it fall?\\n', '<Input>', ' [[0.50, 0.30, 1.10], [\"blockB\"], [0.40, 0.20, 1.30], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] The robot should first grasp block B, then move it slowly and precisely to the edge of the table.\\n[action] [grasp(\"blockB\"), reach([0.80, 0.30, 1.10])]\\n\\n4.\\n', '<Task>', ' Stack block B over block A only if block B is lighter.\\n', '<Instruction>', ' Now, according to the last executed action, which block is lighter? And should it be stacked over the other block?\\n', '<Input>', ' <noinput>\\n', '<Output>', '\\n[verbal] Block B is lighter. It should be stacked over block A.\\n[action] [grasp(\"blockB\"), place(\"blockB\", [0.50, 0.30, 1.15])]\\n\\n5.\\n', '<Task>', ' Flip block A upside down without touching block B.\\n', '<Instruction>', ' What is the best approach to flip block A upside down, considering the task, without accidentally touching block B?\\n', '<Input>', ' [[0.70, 0.20, 1.025], [\"blockA\"], [0.30, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] The robot should grasp block A, lift it, rotate the end effector 180 degrees while in the air, and then place it down again.\\n[action] [grasp(\"blockA\"), move_to([0.70, 0.20, 1.50]), move_to([0.70, 0.20, 1.10])]\\n\\n6.\\n', '<Task>', ' Determine which block is denser based on lifting each block and comparing the force sensor readings.\\n', '<Instruction>', \" What steps should be taken to find out which block is denser, based on the robot's capabilities and the task specifications?\\n\", '<Input>', ' [[0.40, 0.60, 1.50], [<placeholder>], [0.60, 0.30, 1.50], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] The robot should first grasp and lift block A and take the force sensor readings, then repeat the same for block B for comparison.\\n[action] [grasp(\"blockA\"), grasp(\"blockB\")]\\n\\n7.\\n', '<Task>', ' Position the two blocks next to each other in such a way that they touch each other from the largest flat surfaces.\\n', '<Instruction>', ' Given the current state of the blocks, what actions should the robot follow to place the blocks next to each other correctly?\\n', '<Input>', ' [[0.40, 0.50, 1.025], [<placeholder>], [0.50, 0.30, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] The robot should grasp block A, and place it next to block B in a way that they touch each other from the their largest flat surfaces.\\n[action] [grasp(\"blockA\"), place(\"blockA\", [0.56, 0.30, 1.025])]\\n\\n8.\\n', '<Task>', ' Reorder the blocks from left to right on the table based on their weight, with the heavier block on the left.\\n', '<Instruction>', ' Based on the current situation, should the robot first identify the weight of the blocks to reorder them based on their weight?\\n', '<Input>', ' [[0.50, 0.30, 1.025], [\"blockB\"], [0.60, 0.50, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] Yes, the robot needs to first determine the weight of the blocks to decide their desired order.\\n[action] [grasp(\"blockA\"), grasp(\"blockB\")]\\n\\n9.\\n', '<Task>', ' Maximize the distance between the two blocks on the table.\\n', '<Instruction>', ' Based on the given task, what are the next steps for the robot to execute?\\n', '<Input>', ' [[0.50, 0.30, 1.025], [\"blockA\"], [0.60, 0.50, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] The robot should grasp block A and place it on the leftmost edge of the table, while moving block B to the rightmost edge of the table.\\n[action] [grasp(\"blockA\"), place(\"blockA\", [0.10, 0.30, 1.025]), grasp(\"blockB\"), place(\"blockB\", [0.90, 0.50, 1.025])]\\n\\n10.\\n', '<Task>', \" Align the two blocks by one of their sides making sure they don't overlap.\\n\", '<Instruction>', ' What actions should the robot plan to perform to align the blocks without overlapping?\\n', '<Input>', ' [[0.40, 0.50, 1.025], [<placeholder>], [0.60, 0.30, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n', '<Output>', '\\n[verbal] The robot should pick up block B and move it next to block A, making sure they don\\'t overlap.\\n[action] [grasp(\"blockB\"), place(\"blockB\", [0.56, 0.30, 1.025])]\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "new_instructions = post_process_chat_response(ret[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_robot_instruction import old_inst_json_to_gorilla_form_json\n",
    "old_inst_json_to_gorilla_form_json(\"gpt4_generation/instruct_regen.json\", \"gpt4_generation/instruct_regen_gorilla.json\")\n",
    "utils.json2jsonl(\"gpt4_generation/instruct_regen_gorilla.json\", \"gpt4_generation/instruct_regen_gorilla.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_tasks_path=\"./prompts/seeded_tasks.jsonl\"\n",
    "seed_example_path=\"./prompts/seeded_example.jsonl\"\n",
    "function_file_path=\"./prompts/skill_functions.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "seed_tasks = [json.loads(l) for l in open(seed_tasks_path, \"r\")]\n",
    "seed_instructions = [json.loads(l) for l in open(seed_example_path, \"r\")]\n",
    "functions = [json.loads(l) for l in open(function_file_path, \"r\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = []\n",
    "for _ in range(1):\n",
    "    # only sampling from the seed tasks\n",
    "    # prompt_instructions = random.sample(seed_instruction_data, num_prompt_instructions)\n",
    "    prompt = encode_instruct_prompt(\n",
    "        tasks=seed_tasks,\n",
    "        functions=functions,\n",
    "        examples=seed_instructions,\n",
    "    )\n",
    "    batch_input.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### [[{'role': 'user', 'content': 'You are asked to come up with a set of 20 task instructions and corresponding responses. \\nThese task instructions will be given to a GPT model and we will evaluate the GPT model for completing the instructions.\\n\\nThe instruction and response pairs are happening between the robot and a chatbot guider in a robotic environment.\\n\\nHere is the information about the {environment}. \\n\\nThe environment is called \"Two Block World\" \\nThere is a 7DOF Franka robot with a parallel gripper. \\nThere are 2 blocks, {block A} and {block B} with randomized sizes and density in the environment.\\nThe blocks are initialized at a random position on a table.\\nContents in the {} are the names of the objects in the environment.\\n\\nThe robot is given a long horizon task: {TASK}. \\nThe instructions and responses happen when the robot is trying to solve this specific {TASK}. \\n\\nEach instruction data pair consists of three parts: {instruction}, {input}, {output}\\nThe {instruction} consists of the question asked by the robot to help make decisions, it will attach the previous step {instruction}, {input}, {output} at the begining of instruction if it\\'s not the first round of chat.\\n\\nThe {input} consists of the current observation of the robot. If it\\'s not provided, it will be <noinput>.\\n\\nThe {output} consists of two parts <verbal> and <action>.\\n    - The <verbal> part include an description of the reasoning process and the current planned action.\\n    - The <action> part include a downstream action provided in the function lists executable by the robot. \\n\\nBelow is the list of {FUNCTION}s provided in the robot skill library in this environment.\\n\\'\\'\\'\\ndef reach(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of end effector reaching to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef grasp(object_name)\\n\\'\\'\\'\\nThe skill of grasping an object. object_name is the name of the object to be grasped.\\n\\'\\'\\'\\ndef place(object_name, position, orientation[optional])\\n\\'\\'\\'\\nThe skill of placing an object. object_name is the name of the object to be placed. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef move_to(position, orientation[optional])\\n\\'\\'\\'\\nThe skill of moving the end effector to a desired pose. position is a 3D vector and orientation is a quaternion(optional).\\n\\'\\'\\'\\ndef reset()\\n\\'\\'\\'\\nThe skill of resetting the robot to its initial state.\\n\\n\\nBelow is the list of {TASK}s used in the generated instructions can be chosen from a task list:\\n1. {\\'task\\': \\'Identify and pick up the lighter block and move it to the opposite end of the table*.\\'}\\n2. {\\'task\\': \\'Methodically stack block A on top of block B regardless of their weight, in an attempt to maximize their combined height.\\'}\\n3. {\\'task\\': \"Pick and hold block A and shake it gently without dropping it to understand the nature and density of the block\\'s material.*\"}\\n4. {\\'task\\': \\'Using the force sensor, apply a specific amount of force against block B, moving it a certain distance across the table.\\'}\\n\\n\\nHere are some basic requirements for the generated instructions:\\n1. A GPT language model should be able to complete the instruction. For example, do not ask the assistant to create any visual or audio output. For another example, do not ask the assistant to wake you up at 5pm or set a reminder because it cannot perform any action.\\n2. The instructions should be in English.\\n3. The i-th response need to satisfy the following format: \\n// start of instruction pair i, not including this line.\\n###\\ni.\\n<Task> {task}\\n<Instruction> {instruction}\\n<Input> {input}\\n<Output>\\n[verbal] {verbal output}\\n[action] {function output}\\n// end of instruction pair i, not including this line.\\n\\n4. The format of {instruction} will be a question. Usually it will be a question about the next action the robot should take based on the task information and robot observation.\\n5. The format of {input} will be a vector of robot observation. If there are <placeholer> in the input, it means the robot does not provide observation of this state at this step.\\n6. The format of {verbal output} will be a sentence explain the current reasoning process and the current planned action. It is used for in-context learning for multi-turn instructions.\\n7. The format of {action output} will be {function name} {function parameter}. This should be in a python executable form, don\\'t use placeholders as parameters, output the numbers if the parameters are vectors.\\n8. You should generate exact 20 instruction pairs, and make sure they are distributed among different {TASK}s.\\n\\nExamples of instruction pairs are given below. Note that this could be instructions under different {TASK}s. You need to modify the instruction pairs according to the {TASK}.\\n\\n###\\n###\\n###\\n###\\n###\\n###\\n <Task>: stack block A on block B\\n <Instruction>: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input>: [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output>:\\n<verbal output>Yes, the current information is enough to plan and solve the task.\\n<action output>[grasp(blockA), place(blockA, [0.75, 0.5, 1.15])].\\n###\\n <Task>: stack the heavier on the ligher block\\n <Instruction>: Based on the current observation, is it enough to plan and solve the task? If yes, what are the executed skills?\\n <Input>: [[0.20, 0.30, 1.025], [<placeholder>], [0.35, 0.5, 1.025], [<placeholder>], [0.0, 0.0, 1.50], [<placeholder>], [<placeholder>], [<placeholder>]]\\n <Output>:\\n<verbal output>No. We don\\'t know the weight of the blocks, need to infer weight first.\\n<action output>[grasp(blockA), grasp(blockB)].\\n'}]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prompts: 100%|██████████| 1/1 [02:14<00:00, 134.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "decoding_args = utils.OpenAIChatDecodingArguments(\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    max_tokens=2048,\n",
    "    stop=[\"\\n21\", \"21.\"],\n",
    ")\n",
    "chatcompletions = utils.openai_chatcompletion(\n",
    "    prompts=batch_input,\n",
    "    model_name=\"gpt-4\",\n",
    "    decoding_args=decoding_args,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = []\n",
    "for completion in chatcompletions:\n",
    "    new_instructions = post_process_chat_response(completion)\n",
    "    instructions += new_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9\n",
      "['', '[verbal]', ' Based on the density, block A is lighter. The strategy will be to pick up block A and then place it at the position [1.75, 2.75, 0.75].\\n', '[action]', ' reach([1.12, 1.45, 0.75]), grasp(blockA), place(blockA, [1.75, 2.75, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' Yes, the robot should be able to reach block A.\\n', '[action]', ' reach([1.12, 1.45, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' The robot should reach towards block A, grasp it, bring it over block B, then place it on top.\\n', '[action]', ' reach([2, 4, 0.75]), grasp(blockA), place(blockA, [2.5, 3.75, 1.2])']\n",
      "9\n",
      "['', '[verbal]', ' The robot should reach towards block A, grasp it and then shake it. Shaking will be simulated by moving the grasped block back and forth.\\n', '[action]', ' reach([1.75, 2.5, 0.45]), grasp(blockA), move_to([1.75, 2.5, 0.65]), move_to([1.75, 2.5, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' The lighter block, which is Block A, should be placed at the position [1.75, 2.75, 0.75]. \\n', '[action]', ' place(blockA, [1.75, 2.75, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' No, in this case you should reach for block A first as we are stacking it on top of block B.\\n', '[action]', ' reach([2, 4, 0.75])']\n",
      "9\n",
      "['', '[verbal]', ' Block A should be placed at the position [2.5, 3.75, 1.2] to stack it on top of block B.\\n', '[action]', ' place(blockA, [2.5, 3.75, 1.2])']\n",
      "9\n",
      "['', '[verbal]', ' You can reach the block A from its current position which is [1.75, 2.5, 0.45].\\n', '[action]', ' reach([1.75, 2.5, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' To apply force to block B, robot should reach to it, grasp it, and move to the desired position.\\n', '[action]', ' reach([2.5, 3.75, 0.45]), grasp(blockB), move_to([3, 3.75, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' After applying force, block B can be moved to the position [3, 3.75, 0.45].\\n', '[action]', ' move_to([3, 3.75, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' With respect to the provided densities, block A seems to be lighter.\\n', '[action]', ' <noaction>']\n",
      "9\n",
      "['', '[verbal]', ' Yes, we can stack block A on block B without knowing their weights.\\n', '[action]', ' reach([2, 4, 0.75]), grasp(blockA), place(blockA, [2.5, 3.75, 1.2])']\n",
      "9\n",
      "['', '[verbal]', ' As there is no time parameter in the task, the decision will be arbitrary, for example, a two-step shake.\\n', '[action]', ' move_to([1.75, 2.5, 0.65]), move_to([1.75, 2.5, 0.45])']\n",
      "9\n",
      "['', '[verbal]', ' You can use your gripper to apply force and push block B across the table.\\n', '[action]', ' reach([2.5, 3.75, 0.45]), grasp(blockB), move_to([3, 3.75, 0.45])']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "response = completion\n",
    "raw_instructions = response[\"message\"][\"content\"]\n",
    "raw_instructions = re.split(\"###\", raw_instructions)\n",
    "\n",
    "for idx, inst in enumerate(raw_instructions):\n",
    "    # if the decoding stops due to length, the last example is likely truncated so we discard it\n",
    "    if idx == len(raw_instructions) - 1 and response[\"finish_reason\"] == \"length\":\n",
    "        continue\n",
    "    ##### Parse the response into instruction, input, output #####\n",
    "    idx += 1\n",
    "    splitted_data = re.split(f\"(<Task>|<Instruction>|<Input>|<Output>)\", inst)\n",
    "    print(len(splitted_data))\n",
    "    if len(splitted_data) != 9:\n",
    "        continue\n",
    "    else:\n",
    "        task = splitted_data[2].strip()\n",
    "        inst = splitted_data[4].strip()\n",
    "        input = splitted_data[6].strip()\n",
    "        input = \"\" if input.lower() == \"<noinput>\" else input\n",
    "        output = splitted_data[8].strip()\n",
    "        # parse output into <verbal output> and <action output>\n",
    "        output_splitted_data = re.split(\n",
    "            f\"(\\[verbal\\]|\\[action\\])\", output\n",
    "        )\n",
    "        print(output_splitted_data)\n",
    "        verbal_output = output_splitted_data[2].strip()\n",
    "        action_output = output_splitted_data[4].strip()\n",
    "        ##### FILTER OUT Negative Examples #####\n",
    "        # filter out too short or too long instructions\n",
    "        if len(inst.split()) <= 3 or len(inst.split()) > 150:\n",
    "            continue\n",
    "        # filter based on keywords that are not suitable for language models.\n",
    "        # filter those starting with punctuation\n",
    "        if inst[0] in string.punctuation:\n",
    "            continue\n",
    "        # filter those starting with non-english character\n",
    "        if not inst[0].isascii():\n",
    "            continue\n",
    "        instructions.append(\n",
    "            {\n",
    "                \"task\": task,\n",
    "                \"instruction\": inst,\n",
    "                \"input\": input,\n",
    "                \"verbal_output\": verbal_output,\n",
    "                \"action_output\": action_output,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "data = utils.jload(\"./subtask_data/grasp_A_example_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    d = d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1092"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "201e8860b8e6658cdfe301dc13e2a8b5ecdc17e6f5f25d00e326e3c8c2d13630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
